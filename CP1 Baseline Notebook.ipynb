{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP1 Baseline notebook\n",
    "\n",
    "### By Logan Larson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>rt rank best kicker history top maybe top 3. f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>april least one trade made (but one waiting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>giant re-signed restricted free agent guard kevin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                              Tweet\n",
       "0      0  rt rank best kicker history top maybe top 3. f...\n",
       "1      0        april least one trade made (but one waiting\n",
       "2      1  giant re-signed restricted free agent guard kevin"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### import packages ###\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "### load data ###\n",
    "\n",
    "df = pd.read_csv('clean_schefter_tweets')\n",
    "del df['Unnamed: 0']\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "def make_xy(df, vectorizer=None):   \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(df.Tweet.values.astype('U')) # convert object type to unicode\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = (df.Class == 1).values.astype(np.int)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# define cross-validation score\n",
    "def cv_score(clf, X, y, scorefunc):\n",
    "    result = 0.\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(X): # split data into train/test groups, 5 times\n",
    "        clf.fit(X[train], y[train]) # fit the classifier, passed is as clf.\n",
    "        result += scorefunc(clf, X[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average\n",
    "\n",
    "\n",
    "# define log-likelihood score function\n",
    "def log_likelihood(clf, x, y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    irrelevant = y == 0\n",
    "    relevant = ~irrelevant\n",
    "    return prob[irrelevant, 0].sum() + prob[relevant, 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize before train/test split\n",
    "X, y = make_xy(df)\n",
    "\n",
    "# split dataset into a training and test set\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show train/test classification for both models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Naive Bayes classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.93     19185\n",
      "           1       0.70      0.88      0.78      5595\n",
      "\n",
      "    accuracy                           0.89     24780\n",
      "   macro avg       0.83      0.89      0.85     24780\n",
      "weighted avg       0.90      0.89      0.89     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92      6391\n",
      "           1       0.69      0.85      0.76      1870\n",
      "\n",
      "    accuracy                           0.88      8261\n",
      "   macro avg       0.82      0.87      0.84      8261\n",
      "weighted avg       0.89      0.88      0.88      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB = BernoulliNB().fit(xtrain, ytrain)\n",
    "NBtrain_pred = NB.predict(xtrain)\n",
    "NBtest_pred = NB.predict(xtest)\n",
    "\n",
    "print('\\n Naive Bayes classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, NBtrain_pred, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, NBtest_pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96     19185\n",
      "           1       0.90      0.82      0.86      5595\n",
      "\n",
      "    accuracy                           0.94     24780\n",
      "   macro avg       0.92      0.90      0.91     24780\n",
      "weighted avg       0.94      0.94      0.94     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94      6391\n",
      "           1       0.84      0.75      0.79      1870\n",
      "\n",
      "    accuracy                           0.91      8261\n",
      "   macro avg       0.88      0.85      0.87      8261\n",
      "weighted avg       0.91      0.91      0.91      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression().fit(xtrain, ytrain)\n",
    "LR_train_pred = LR.predict(xtrain)\n",
    "LR_test_pred = LR.predict(xtest)\n",
    "\n",
    "print('\\n Logistic Regression classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, LR_train_pred, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, LR_test_pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine if there is overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For naive bayes model: \n",
    "    - Apparent overfitting in terms of precision for both test and training sets\n",
    "    - Meanwhile, performs well in terms of recall and overall accuracy\n",
    "    \n",
    "For logistic regression model:\n",
    "    - Interestingly, the opposite seems to be the case for this model\n",
    "    - Performs well in terms of precision, but overfit in terms of recall\n",
    "    \n",
    "Since overfitting is present in both, I'll proceed with regularized models for both naive bayes and logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build regularized models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 5\n"
     ]
    }
   ],
   "source": [
    "# define the grid of parameters to search over\n",
    "alphas = [.01, .1, 1, 5, 10, 50, 100]\n",
    "min_df = 0.001\n",
    "\n",
    "# find the best value for alpha\n",
    "best_alpha = None\n",
    "_, itest = train_test_split(range(df.shape[0]))\n",
    "mask = np.zeros(df.shape[0], dtype=np.bool)\n",
    "mask[itest] = True\n",
    "maxscore = -np.inf\n",
    "for alpha in alphas:        \n",
    "    vectorizer = CountVectorizer(min_df = min_df)       \n",
    "    Xthis, ythis = make_xy(df, vectorizer)\n",
    "    Xtrainthis = Xthis[mask]\n",
    "    ytrainthis = ythis[mask]\n",
    "    \n",
    "    clf = BernoulliNB(alpha=alpha)\n",
    "    \n",
    "    cvscore = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "    \n",
    "    if cvscore > maxscore:\n",
    "        maxscore = cvscore\n",
    "        best_alpha = alpha\n",
    "        \n",
    "print('Optimal alpha: {}'.format(best_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tuned Naive Bayes classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     19185\n",
      "           1       0.73      0.81      0.77      5595\n",
      "\n",
      "    accuracy                           0.89     24780\n",
      "   macro avg       0.84      0.86      0.85     24780\n",
      "weighted avg       0.90      0.89      0.89     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      6391\n",
      "           1       0.72      0.78      0.75      1870\n",
      "\n",
      "    accuracy                           0.88      8261\n",
      "   macro avg       0.83      0.85      0.84      8261\n",
      "weighted avg       0.89      0.88      0.88      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_tuned = BernoulliNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "NB_tuned_train_pred = NB_tuned.predict(xtrain)\n",
    "NB_tuned_test_pred = NB_tuned.predict(xtest)\n",
    "\n",
    "print('\\n Tuned Naive Bayes classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, NB_tuned_train_pred, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, NB_tuned_test_pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared to non-regularized model:\n",
    "\n",
    "- Less overfit in terms of precision, but more overfit in terms of recall\n",
    "- Overall accuracy barely changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal C value: 1\n",
      "\n",
      " Tuned Logistic Regression classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96     19185\n",
      "           1       0.90      0.82      0.86      5595\n",
      "\n",
      "    accuracy                           0.94     24780\n",
      "   macro avg       0.92      0.90      0.91     24780\n",
      "weighted avg       0.94      0.94      0.94     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94      6391\n",
      "           1       0.84      0.75      0.79      1870\n",
      "\n",
      "    accuracy                           0.91      8261\n",
      "   macro avg       0.88      0.85      0.87      8261\n",
      "weighted avg       0.91      0.91      0.91      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hypertuning C parameter\n",
    "LR2 = LogisticRegression()\n",
    "parameters = {\"C\": [0.0001, 0.001, 0.1, 1, 2, 3, 4, 5, 10, 100, 1000, 10000]}\n",
    "fitmodel = GridSearchCV(LR2, param_grid=parameters, cv=10, scoring=\"accuracy\").fit(xtrain,ytrain)\n",
    "fitmodel.best_params_, fitmodel.best_score_, fitmodel.cv_results_\n",
    "\n",
    "LR_tuned = LogisticRegression(C=fitmodel.best_params_['C']).fit(xtrain, ytrain)\n",
    "LR_tuned_train_pred = LR_tuned.predict(xtrain)\n",
    "LR_tuned_test_pred = LR_tuned.predict(xtest)\n",
    "\n",
    "print('Optimal C value:', fitmodel.best_params_['C'])\n",
    "print('\\n Tuned Logistic Regression classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, LR_tuned_train_pred, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, LR_tuned_test_pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared to non-regularized model\n",
    "\n",
    "Since the optimal alpha turned out to be the same as the default setting (1), there is no difference between regularized and non-regularized models. Let's instead look at a ridge and lasso classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression classifier with L1 penalty: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96     19185\n",
      "           1       0.88      0.80      0.84      5595\n",
      "\n",
      "    accuracy                           0.93     24780\n",
      "   macro avg       0.91      0.89      0.90     24780\n",
      "weighted avg       0.93      0.93      0.93     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94      6391\n",
      "           1       0.84      0.75      0.79      1870\n",
      "\n",
      "    accuracy                           0.91      8261\n",
      "   macro avg       0.89      0.85      0.87      8261\n",
      "weighted avg       0.91      0.91      0.91      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lasso\n",
    "\n",
    "lasso = LogisticRegression(C=1, penalty='l1').fit(xtrain,ytrain)\n",
    "\n",
    "lasso_train_pred = lasso.predict(xtrain)\n",
    "lasso_test_pred = lasso.predict(xtest)\n",
    "\n",
    "print('\\n Logistic Regression classifier with L1 penalty: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, lasso_train_pred, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, lasso_test_pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared to non-regularized model\n",
    "\n",
    "Performs the same as the baseline model on test data, but performs worse on all metrics on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression classifier with L2 penalty: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96     19185\n",
      "           1       0.90      0.82      0.86      5595\n",
      "\n",
      "    accuracy                           0.94     24780\n",
      "   macro avg       0.92      0.90      0.91     24780\n",
      "weighted avg       0.94      0.94      0.94     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94      6391\n",
      "           1       0.84      0.75      0.79      1870\n",
      "\n",
      "    accuracy                           0.91      8261\n",
      "   macro avg       0.88      0.85      0.87      8261\n",
      "weighted avg       0.91      0.91      0.91      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ridge\n",
    "\n",
    "ridge = LogisticRegression(C=1, penalty='l2').fit(xtrain,ytrain)\n",
    "\n",
    "ridge_train_pred = ridge.predict(xtrain)\n",
    "ridge_test_pred = ridge.predict(xtest)\n",
    "\n",
    "print('\\n Logistic Regression classifier with L2 penalty: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, ridge_train_pred, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, ridge_test_pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared to non-regularized model:\n",
    "\n",
    "Performs the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which metric aligns with the business problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- False positive: irrelevant tweet classified as relevant\n",
    "- False negative: relevant tweet classified as irrelevant\n",
    "\n",
    "False negatives are more acceptable in this problem, so I'm better served by prioritizing precision (TP/TP+FP) over recall (TP/TP+FN). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              NB    NB (tuned)    LR    LR (tuned)    Ridge    Lasso\n",
      "----------------------  --------  ------------  ----  ------------  -------  -------\n",
      "Training Precision (0)      0.96          0.94  0.95          0.95     0.95     0.94\n",
      "Training Precision (1)      0.7           0.73  0.9           0.9      0.9      0.88\n",
      "Test Precision (0)          0.95          0.93  0.93          0.93     0.93     0.93\n",
      "Test Precision (1)          0.69          0.72  0.84          0.84     0.84     0.84\n",
      "Training Recall (0)         0.89          0.92  0.97          0.97     0.97     0.97\n",
      "Training Recall (1)         0.88          0.81  0.82          0.82     0.82     0.8\n",
      "Test Recall (0)             0.89          0.91  0.96          0.96     0.96     0.96\n",
      "Test Recall (1)             0.85          0.78  0.75          0.75     0.75     0.75\n",
      "Training F1 score (0)       0.93          0.93  0.96          0.96     0.96     0.96\n",
      "Training F1 score (1)       0.78          0.77  0.86          0.86     0.86     0.84\n",
      "Test F1 score (0)           0.92          0.92  0.94          0.94     0.94     0.94\n",
      "Test F1 score (1)           0.76          0.75  0.79          0.79     0.79     0.79\n",
      "Count - Training (0)    19185\n",
      "Count - Training (1)     5595\n",
      "Count - Test (0)         6391\n",
      "Count - Test (1)         1870\n"
     ]
    }
   ],
   "source": [
    "# summary table \n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate([\n",
    "          ['Training Precision (0)', 0.96, 0.94, 0.95, 0.95, 0.95, 0.94],\n",
    "          ['Training Precision (1)', 0.70, 0.73, 0.90, 0.90, 0.90, 0.88],\n",
    "          ['Test Precision (0)', 0.95, 0.93, 0.93, 0.93, 0.93, 0.93],\n",
    "          ['Test Precision (1)', 0.69, 0.72, 0.84, 0.84, 0.84, 0.84],\n",
    "          ['Training Recall (0)', 0.89, 0.92, 0.97, 0.97, 0.97, 0.97],\n",
    "          ['Training Recall (1)', 0.88, 0.81, 0.82, 0.82, 0.82, 0.80],\n",
    "          ['Test Recall (0)', 0.89, 0.91, 0.96, 0.96, 0.96, 0.96],\n",
    "          ['Test Recall (1)', 0.85, 0.78, 0.75, 0.75, 0.75, 0.75],\n",
    "          ['Training F1 score (0)', 0.93, 0.93, 0.96, 0.96, 0.96, 0.96],\n",
    "          ['Training F1 score (1)', 0.78, 0.77, 0.86, 0.86, 0.86, 0.84],\n",
    "          ['Test F1 score (0)', 0.92, 0.92, 0.94, 0.94, 0.94, 0.94],\n",
    "          ['Test F1 score (1)', 0.76, 0.75, 0.79, 0.79, 0.79, 0.79],\n",
    "                ['Count - Training (0)', 19185],\n",
    "                ['Count - Training (1)', 5595],\n",
    "                ['Count - Test (0)', 6391],\n",
    "                ['Count - Test (1)', 1870]\n",
    "         ], \n",
    "               headers=['', 'NB', 'NB (tuned)', 'LR', 'LR (tuned)', 'Ridge', 'Lasso']\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In prioritizing precision, my choice is to use the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
