{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Springboard Data Science Career Track\n",
    "\n",
    "## Capstone Project #2: Learning Neural Networks through Text Generation\n",
    "\n",
    "## Extended Model #11: Further Increasing Number of Epochs of Word-Level Model\n",
    "\n",
    "##### By Logan Larson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "//miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "//miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "//miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "//miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "//miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "//miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "//miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "//miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "//miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "//miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "//miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "//miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from numpy import array\n",
    "import string\n",
    "import re\n",
    "from random import randint\n",
    "from pickle import load, dump\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical, np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Embedding\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To Sherlock Holmes she is always THE woman. I have seldom heard\n",
      "him mention her under any other name. \n"
     ]
    }
   ],
   "source": [
    "# load text and convert to lowercase\n",
    "filename = \"AdventuresOfSherlockHolmes.txt\"\n",
    "raw_text = open(filename).read()\n",
    "print(raw_text[:102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'sherlock', 'holmes', 'she', 'is', 'always', 'the', 'woman', 'i', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name']\n"
     ]
    }
   ],
   "source": [
    "# convert to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "# replace dashes\n",
    "raw_text = raw_text.replace('--', '')\n",
    "# split words by white space\n",
    "tokens = raw_text.split()\n",
    "# remove punctuation from each word\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "tokens = [re_punc.sub('', w) for w in tokens]\n",
    "# remove any non-alphabetic tokens\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "print(tokens[:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104202"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  104194\n"
     ]
    }
   ],
   "source": [
    "seq_length = 8\n",
    "words = sorted(list(set(tokens)))\n",
    "token_to_int = dict((c, i) for i, c in enumerate(words))\n",
    "dataX = []\n",
    "dataY = []\n",
    "n_tokens = len(tokens)\n",
    "for i in range(0, n_tokens - seq_length, 1):\n",
    "    seq_in = tokens[i:i + seq_length]\n",
    "    seq_out = tokens[i + seq_length]\n",
    "    dataX.append([token_to_int[token] for token in seq_in])\n",
    "    dataY.append(token_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 1\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(words))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "print(X.shape[1], X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/60\n",
      "104194/104194 [==============================] - 98s 936us/step - loss: 6.6110\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.61104, saving model to weights-improvement-01-6.6110.hdf5\n",
      "Epoch 2/60\n",
      "104194/104194 [==============================] - 101s 969us/step - loss: 6.4574\n",
      "\n",
      "Epoch 00002: loss improved from 6.61104 to 6.45745, saving model to weights-improvement-02-6.4574.hdf5\n",
      "Epoch 3/60\n",
      "104194/104194 [==============================] - 107s 1ms/step - loss: 6.4470\n",
      "\n",
      "Epoch 00003: loss improved from 6.45745 to 6.44698, saving model to weights-improvement-03-6.4470.hdf5\n",
      "Epoch 4/60\n",
      "104194/104194 [==============================] - 105s 1ms/step - loss: 6.4416\n",
      "\n",
      "Epoch 00004: loss improved from 6.44698 to 6.44162, saving model to weights-improvement-04-6.4416.hdf5\n",
      "Epoch 5/60\n",
      "104194/104194 [==============================] - 106s 1ms/step - loss: 6.4357\n",
      "\n",
      "Epoch 00005: loss improved from 6.44162 to 6.43574, saving model to weights-improvement-05-6.4357.hdf5\n",
      "Epoch 6/60\n",
      "104194/104194 [==============================] - 106s 1ms/step - loss: 6.4261\n",
      "\n",
      "Epoch 00006: loss improved from 6.43574 to 6.42614, saving model to weights-improvement-06-6.4261.hdf5\n",
      "Epoch 7/60\n",
      "104194/104194 [==============================] - 102s 979us/step - loss: 6.4131\n",
      "\n",
      "Epoch 00007: loss improved from 6.42614 to 6.41306, saving model to weights-improvement-07-6.4131.hdf5\n",
      "Epoch 8/60\n",
      "104194/104194 [==============================] - 103s 993us/step - loss: 6.3962\n",
      "\n",
      "Epoch 00008: loss improved from 6.41306 to 6.39623, saving model to weights-improvement-08-6.3962.hdf5\n",
      "Epoch 9/60\n",
      "104194/104194 [==============================] - 103s 990us/step - loss: 6.3753\n",
      "\n",
      "Epoch 00009: loss improved from 6.39623 to 6.37527, saving model to weights-improvement-09-6.3753.hdf5\n",
      "Epoch 10/60\n",
      "104194/104194 [==============================] - 101s 965us/step - loss: 6.3488\n",
      "\n",
      "Epoch 00010: loss improved from 6.37527 to 6.34884, saving model to weights-improvement-10-6.3488.hdf5\n",
      "Epoch 11/60\n",
      "104194/104194 [==============================] - 96s 924us/step - loss: 6.3198\n",
      "\n",
      "Epoch 00011: loss improved from 6.34884 to 6.31981, saving model to weights-improvement-11-6.3198.hdf5\n",
      "Epoch 12/60\n",
      "104194/104194 [==============================] - 95s 911us/step - loss: 6.2883\n",
      "\n",
      "Epoch 00012: loss improved from 6.31981 to 6.28830, saving model to weights-improvement-12-6.2883.hdf5\n",
      "Epoch 13/60\n",
      "104194/104194 [==============================] - 95s 910us/step - loss: 6.2506\n",
      "\n",
      "Epoch 00013: loss improved from 6.28830 to 6.25057, saving model to weights-improvement-13-6.2506.hdf5\n",
      "Epoch 14/60\n",
      "104194/104194 [==============================] - 95s 911us/step - loss: 6.2070\n",
      "\n",
      "Epoch 00014: loss improved from 6.25057 to 6.20705, saving model to weights-improvement-14-6.2070.hdf5\n",
      "Epoch 15/60\n",
      "104194/104194 [==============================] - 99s 954us/step - loss: 6.1567\n",
      "\n",
      "Epoch 00015: loss improved from 6.20705 to 6.15665, saving model to weights-improvement-15-6.1567.hdf5\n",
      "Epoch 16/60\n",
      "104194/104194 [==============================] - 101s 969us/step - loss: 6.0976\n",
      "\n",
      "Epoch 00016: loss improved from 6.15665 to 6.09764, saving model to weights-improvement-16-6.0976.hdf5\n",
      "Epoch 17/60\n",
      "104194/104194 [==============================] - 96s 925us/step - loss: 6.0259\n",
      "\n",
      "Epoch 00017: loss improved from 6.09764 to 6.02589, saving model to weights-improvement-17-6.0259.hdf5\n",
      "Epoch 18/60\n",
      "104194/104194 [==============================] - 94s 904us/step - loss: 5.9477\n",
      "\n",
      "Epoch 00018: loss improved from 6.02589 to 5.94771, saving model to weights-improvement-18-5.9477.hdf5\n",
      "Epoch 19/60\n",
      "104194/104194 [==============================] - 95s 912us/step - loss: 5.8603\n",
      "\n",
      "Epoch 00019: loss improved from 5.94771 to 5.86027, saving model to weights-improvement-19-5.8603.hdf5\n",
      "Epoch 20/60\n",
      "104194/104194 [==============================] - 95s 913us/step - loss: 5.7694\n",
      "\n",
      "Epoch 00020: loss improved from 5.86027 to 5.76939, saving model to weights-improvement-20-5.7694.hdf5\n",
      "Epoch 21/60\n",
      "104194/104194 [==============================] - 98s 943us/step - loss: 5.6766\n",
      "\n",
      "Epoch 00021: loss improved from 5.76939 to 5.67659, saving model to weights-improvement-21-5.6766.hdf5\n",
      "Epoch 22/60\n",
      "104194/104194 [==============================] - 98s 943us/step - loss: 5.5880\n",
      "\n",
      "Epoch 00022: loss improved from 5.67659 to 5.58805, saving model to weights-improvement-22-5.5880.hdf5\n",
      "Epoch 23/60\n",
      "104194/104194 [==============================] - 101s 972us/step - loss: 5.4951\n",
      "\n",
      "Epoch 00023: loss improved from 5.58805 to 5.49506, saving model to weights-improvement-23-5.4951.hdf5\n",
      "Epoch 24/60\n",
      "104194/104194 [==============================] - 104s 995us/step - loss: 5.4110\n",
      "\n",
      "Epoch 00024: loss improved from 5.49506 to 5.41095, saving model to weights-improvement-24-5.4110.hdf5\n",
      "Epoch 25/60\n",
      "104194/104194 [==============================] - 103s 986us/step - loss: 5.3277\n",
      "\n",
      "Epoch 00025: loss improved from 5.41095 to 5.32775, saving model to weights-improvement-25-5.3277.hdf5\n",
      "Epoch 26/60\n",
      "104194/104194 [==============================] - 100s 962us/step - loss: 5.2523\n",
      "\n",
      "Epoch 00026: loss improved from 5.32775 to 5.25227, saving model to weights-improvement-26-5.2523.hdf5\n",
      "Epoch 27/60\n",
      "104194/104194 [==============================] - 103s 987us/step - loss: 5.1735\n",
      "\n",
      "Epoch 00027: loss improved from 5.25227 to 5.17352, saving model to weights-improvement-27-5.1735.hdf5\n",
      "Epoch 28/60\n",
      "104194/104194 [==============================] - 98s 941us/step - loss: 5.1092\n",
      "\n",
      "Epoch 00028: loss improved from 5.17352 to 5.10920, saving model to weights-improvement-28-5.1092.hdf5\n",
      "Epoch 29/60\n",
      "104194/104194 [==============================] - 97s 931us/step - loss: 5.0366\n",
      "\n",
      "Epoch 00029: loss improved from 5.10920 to 5.03656, saving model to weights-improvement-29-5.0366.hdf5\n",
      "Epoch 30/60\n",
      "104194/104194 [==============================] - 97s 930us/step - loss: 4.9657\n",
      "\n",
      "Epoch 00030: loss improved from 5.03656 to 4.96574, saving model to weights-improvement-30-4.9657.hdf5\n",
      "Epoch 31/60\n",
      "104194/104194 [==============================] - 100s 961us/step - loss: 4.9011\n",
      "\n",
      "Epoch 00031: loss improved from 4.96574 to 4.90108, saving model to weights-improvement-31-4.9011.hdf5\n",
      "Epoch 32/60\n",
      "104194/104194 [==============================] - 103s 987us/step - loss: 4.8376\n",
      "\n",
      "Epoch 00032: loss improved from 4.90108 to 4.83763, saving model to weights-improvement-32-4.8376.hdf5\n",
      "Epoch 33/60\n",
      "104194/104194 [==============================] - 104s 1ms/step - loss: 4.7782\n",
      "\n",
      "Epoch 00033: loss improved from 4.83763 to 4.77817, saving model to weights-improvement-33-4.7782.hdf5\n",
      "Epoch 34/60\n",
      "104194/104194 [==============================] - 98s 943us/step - loss: 4.7167\n",
      "\n",
      "Epoch 00034: loss improved from 4.77817 to 4.71668, saving model to weights-improvement-34-4.7167.hdf5\n",
      "Epoch 35/60\n",
      "104194/104194 [==============================] - 95s 913us/step - loss: 4.6651\n",
      "\n",
      "Epoch 00035: loss improved from 4.71668 to 4.66511, saving model to weights-improvement-35-4.6651.hdf5\n",
      "Epoch 36/60\n",
      "104194/104194 [==============================] - 96s 919us/step - loss: 4.6095\n",
      "\n",
      "Epoch 00036: loss improved from 4.66511 to 4.60946, saving model to weights-improvement-36-4.6095.hdf5\n",
      "Epoch 37/60\n",
      "104194/104194 [==============================] - 96s 922us/step - loss: 4.5601\n",
      "\n",
      "Epoch 00037: loss improved from 4.60946 to 4.56010, saving model to weights-improvement-37-4.5601.hdf5\n",
      "Epoch 38/60\n",
      "104194/104194 [==============================] - 94s 907us/step - loss: 4.5100\n",
      "\n",
      "Epoch 00038: loss improved from 4.56010 to 4.51002, saving model to weights-improvement-38-4.5100.hdf5\n",
      "Epoch 39/60\n",
      "104194/104194 [==============================] - 93s 897us/step - loss: 4.4595\n",
      "\n",
      "Epoch 00039: loss improved from 4.51002 to 4.45948, saving model to weights-improvement-39-4.4595.hdf5\n",
      "Epoch 40/60\n",
      "104194/104194 [==============================] - 94s 904us/step - loss: 4.4164\n",
      "\n",
      "Epoch 00040: loss improved from 4.45948 to 4.41641, saving model to weights-improvement-40-4.4164.hdf5\n",
      "Epoch 41/60\n",
      "104194/104194 [==============================] - 93s 894us/step - loss: 4.3608\n",
      "\n",
      "Epoch 00041: loss improved from 4.41641 to 4.36077, saving model to weights-improvement-41-4.3608.hdf5\n",
      "Epoch 42/60\n",
      "104194/104194 [==============================] - 93s 891us/step - loss: 4.3244\n",
      "\n",
      "Epoch 00042: loss improved from 4.36077 to 4.32441, saving model to weights-improvement-42-4.3244.hdf5\n",
      "Epoch 43/60\n",
      "104194/104194 [==============================] - 94s 900us/step - loss: 4.2828\n",
      "\n",
      "Epoch 00043: loss improved from 4.32441 to 4.28279, saving model to weights-improvement-43-4.2828.hdf5\n",
      "Epoch 44/60\n",
      "104194/104194 [==============================] - 94s 901us/step - loss: 4.2371\n",
      "\n",
      "Epoch 00044: loss improved from 4.28279 to 4.23712, saving model to weights-improvement-44-4.2371.hdf5\n",
      "Epoch 45/60\n",
      "104194/104194 [==============================] - 92s 885us/step - loss: 4.2028\n",
      "\n",
      "Epoch 00045: loss improved from 4.23712 to 4.20283, saving model to weights-improvement-45-4.2028.hdf5\n",
      "Epoch 46/60\n",
      "104194/104194 [==============================] - 92s 884us/step - loss: 4.1675\n",
      "\n",
      "Epoch 00046: loss improved from 4.20283 to 4.16750, saving model to weights-improvement-46-4.1675.hdf5\n",
      "Epoch 47/60\n",
      "104194/104194 [==============================] - 92s 886us/step - loss: 4.1376\n",
      "\n",
      "Epoch 00047: loss improved from 4.16750 to 4.13761, saving model to weights-improvement-47-4.1376.hdf5\n",
      "Epoch 48/60\n",
      "104194/104194 [==============================] - 92s 882us/step - loss: 4.0938\n",
      "\n",
      "Epoch 00048: loss improved from 4.13761 to 4.09382, saving model to weights-improvement-48-4.0938.hdf5\n",
      "Epoch 49/60\n",
      "104194/104194 [==============================] - 91s 871us/step - loss: 4.0557\n",
      "\n",
      "Epoch 00049: loss improved from 4.09382 to 4.05565, saving model to weights-improvement-49-4.0557.hdf5\n",
      "Epoch 50/60\n",
      "104194/104194 [==============================] - 91s 872us/step - loss: 4.0230\n",
      "\n",
      "Epoch 00050: loss improved from 4.05565 to 4.02300, saving model to weights-improvement-50-4.0230.hdf5\n",
      "Epoch 51/60\n",
      "104194/104194 [==============================] - 91s 872us/step - loss: 3.9932\n",
      "\n",
      "Epoch 00051: loss improved from 4.02300 to 3.99318, saving model to weights-improvement-51-3.9932.hdf5\n",
      "Epoch 52/60\n",
      "104194/104194 [==============================] - 91s 873us/step - loss: 3.9627\n",
      "\n",
      "Epoch 00052: loss improved from 3.99318 to 3.96267, saving model to weights-improvement-52-3.9627.hdf5\n",
      "Epoch 53/60\n",
      "104194/104194 [==============================] - 92s 882us/step - loss: 3.9282\n",
      "\n",
      "Epoch 00053: loss improved from 3.96267 to 3.92817, saving model to weights-improvement-53-3.9282.hdf5\n",
      "Epoch 54/60\n",
      "104194/104194 [==============================] - 91s 869us/step - loss: 3.9020\n",
      "\n",
      "Epoch 00054: loss improved from 3.92817 to 3.90201, saving model to weights-improvement-54-3.9020.hdf5\n",
      "Epoch 55/60\n",
      "104194/104194 [==============================] - 90s 866us/step - loss: 3.8747\n",
      "\n",
      "Epoch 00055: loss improved from 3.90201 to 3.87468, saving model to weights-improvement-55-3.8747.hdf5\n",
      "Epoch 56/60\n",
      "104194/104194 [==============================] - 90s 862us/step - loss: 3.8397\n",
      "\n",
      "Epoch 00056: loss improved from 3.87468 to 3.83969, saving model to weights-improvement-56-3.8397.hdf5\n",
      "Epoch 57/60\n",
      "104194/104194 [==============================] - 90s 861us/step - loss: 3.8107\n",
      "\n",
      "Epoch 00057: loss improved from 3.83969 to 3.81066, saving model to weights-improvement-57-3.8107.hdf5\n",
      "Epoch 58/60\n",
      "104194/104194 [==============================] - 97s 931us/step - loss: 3.7886\n",
      "\n",
      "Epoch 00058: loss improved from 3.81066 to 3.78859, saving model to weights-improvement-58-3.7886.hdf5\n",
      "Epoch 59/60\n",
      "104194/104194 [==============================] - 97s 927us/step - loss: 3.7562\n",
      "\n",
      "Epoch 00059: loss improved from 3.78859 to 3.75623, saving model to weights-improvement-59-3.7562.hdf5\n",
      "Epoch 60/60\n",
      "104194/104194 [==============================] - 97s 928us/step - loss: 3.7292\n",
      "\n",
      "Epoch 00060: loss improved from 3.75623 to 3.72915, saving model to weights-improvement-60-3.7292.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x62a8200b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=60, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-60-3.7292.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" your commonplace featureless crimes which are really puzzling \"\n",
      "the a i manner of the the stock i the the and buried i the planet of the the states and the mostly concerned to coronet the the the the holmes the the the and the repeat war entreaties disappeared the the cried the the reasoned and the lazily the guilty assertion shattered the hosmer roads marseilles prolonged absence described caused the advise snowclad the stretched floor of a the tobacco the gate of the the rabbit and the the and contemplative purveyor papers side gripping the a the chamber the figure of the fields the the wont unhappy must \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "int_to_token = dict((i, c) for i, c in enumerate(words))\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ' '.join([int_to_token[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(words))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_token[index]\n",
    "    seq_in = [int_to_token[value] for value in pattern]\n",
    "    sys.stdout.write(result + \" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" there he was still with a kind of \"\n",
      "the the coincidences to i the the the the eyford the the supplied a the lazily the bohemia and regurgitation of cornwall the waiting of the the of the the brightly dwelling the the the wheeled demand the the the the cover the the the the cover the the the the cover the the the the cover the the the the cover the the the the cover the the the the cover the the the the cover the the the the cover the the the the cover the the the the cover the the the the cover the the the \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start2 = numpy.random.randint(576, len(dataX)-1)\n",
    "pattern = dataX[start2]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ' '.join([int_to_token[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(words))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_token[index]\n",
    "    seq_in = [int_to_token[value] for value in pattern]\n",
    "    sys.stdout.write(result + \" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" i do so want a little help why \"\n",
      "a and the i of the and at it i to world frenzy the alpha the the and the whose skylight following the i the the the recognize was the wagons the the the the the have of the tension i a i the the mto the results naturally he confessed the the the i of the the suit a the the hereford of the i amusing i the the a the corridor of the the precisely abutted drunkards narrative implicate the the and the ate the afternoon each forth i the the the the are and the i the \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start3 = numpy.random.randint(1291, len(dataX)-1)\n",
    "pattern = dataX[start3]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ' '.join([int_to_token[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(words))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_token[index]\n",
    "    seq_in = [int_to_token[value] for value in pattern]\n",
    "    sys.stdout.write(result + \" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" consequences to me far from hushing the thing \"\n",
      "business comfortablelooking the the a and the method of answering the the of the is the a the murdering the the providing was kitchen purport the the suggestiveness a pitch pile expectancy the the the depressing the the the afterwards the lazily of the gold the the wednesday the permission to the chubb the the the of english illuminated the the pleased the law the the and it and the to the had please afterwards the the the the is the the the the county of the the sisters and sounded busy the the tied will be the scattered bred \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start4 = numpy.random.randint(291, len(dataX)-1)\n",
    "pattern = dataX[start4]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ' '.join([int_to_token[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(words))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_token[index]\n",
    "    seq_in = [int_to_token[value] for value in pattern]\n",
    "    sys.stdout.write(result + \" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" every way that they should make their position \"\n",
      "the saucer the milk the the the and the mostly faced the coronet the the the have the a to the the rain a probing the sufficient punishment the i the the the and the securea war entreaties shook the of the you the the the the the justified of the the the a the the hereford of the i amusing i the the a the corridor of the the precisely abutted drunkards narrative implicate the the and the ate the afternoon each forth i the the the the are and the i the the i a the i the \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start5 = numpy.random.randint(2291, len(dataX)-1)\n",
    "pattern = dataX[start5]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ' '.join([int_to_token[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(words))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_token[index]\n",
    "    seq_in = [int_to_token[value] for value in pattern]\n",
    "    sys.stdout.write(result + \" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
