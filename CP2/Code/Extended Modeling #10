{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Springboard Data Science Career Track\n",
    "\n",
    "## Capstone Project #2: Learning Neural Networks through Text Generation\n",
    "\n",
    "## Extended Model #10: Changing Activation Function of Word-Level Model\n",
    "\n",
    "##### By Logan Larson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy import array\n",
    "import string\n",
    "import re\n",
    "from random import randint\n",
    "from pickle import load, dump\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical, np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To Sherlock Holmes she is always THE woman. I have seldom heard\n",
      "him mention her under any other name. \n"
     ]
    }
   ],
   "source": [
    "# load text and convert to lowercase\n",
    "filename = \"AdventuresOfSherlockHolmes.txt\"\n",
    "raw_text = open(filename).read()\n",
    "print(raw_text[:102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'sherlock', 'holmes', 'she', 'is', 'always', 'the', 'woman', 'i', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name']\n"
     ]
    }
   ],
   "source": [
    "# convert to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "# replace dashes\n",
    "raw_text = raw_text.replace('--', '')\n",
    "# split words by white space\n",
    "tokens = raw_text.split()\n",
    "# remove punctuation from each word\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "tokens = [re_punc.sub('', w) for w in tokens]\n",
    "# remove any non-alphabetic tokens\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "print(tokens[:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104202"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  104194\n"
     ]
    }
   ],
   "source": [
    "seq_length = 8\n",
    "words = sorted(list(set(tokens)))\n",
    "token_to_int = dict((c, i) for i, c in enumerate(words))\n",
    "dataX = []\n",
    "dataY = []\n",
    "n_tokens = len(tokens)\n",
    "for i in range(0, n_tokens - seq_length, 1):\n",
    "    seq_in = tokens[i:i + seq_length]\n",
    "    seq_out = tokens[i + seq_length]\n",
    "    dataX.append([token_to_int[token] for token in seq_in])\n",
    "    dataY.append(token_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 1\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(words))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "print(X.shape[1], X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/40\n",
      "104194/104194 [==============================] - 94s 903us/step - loss: 6.6101\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.61009, saving model to weights-improvement-01-6.6101.hdf5\n",
      "Epoch 2/40\n",
      "104194/104194 [==============================] - 94s 899us/step - loss: 6.4564\n",
      "\n",
      "Epoch 00002: loss improved from 6.61009 to 6.45640, saving model to weights-improvement-02-6.4564.hdf5\n",
      "Epoch 3/40\n",
      "104194/104194 [==============================] - 95s 910us/step - loss: 6.4472\n",
      "\n",
      "Epoch 00003: loss improved from 6.45640 to 6.44716, saving model to weights-improvement-03-6.4472.hdf5\n",
      "Epoch 4/40\n",
      "104194/104194 [==============================] - 98s 944us/step - loss: 6.4420\n",
      "\n",
      "Epoch 00004: loss improved from 6.44716 to 6.44195, saving model to weights-improvement-04-6.4420.hdf5\n",
      "Epoch 5/40\n",
      "104194/104194 [==============================] - 113s 1ms/step - loss: 6.4373\n",
      "\n",
      "Epoch 00005: loss improved from 6.44195 to 6.43728, saving model to weights-improvement-05-6.4373.hdf5\n",
      "Epoch 6/40\n",
      "104194/104194 [==============================] - 102s 982us/step - loss: 6.4249\n",
      "\n",
      "Epoch 00006: loss improved from 6.43728 to 6.42489, saving model to weights-improvement-06-6.4249.hdf5\n",
      "Epoch 7/40\n",
      "104194/104194 [==============================] - 98s 941us/step - loss: 6.4108\n",
      "\n",
      "Epoch 00007: loss improved from 6.42489 to 6.41085, saving model to weights-improvement-07-6.4108.hdf5\n",
      "Epoch 8/40\n",
      "104194/104194 [==============================] - 95s 911us/step - loss: 6.3931\n",
      "\n",
      "Epoch 00008: loss improved from 6.41085 to 6.39305, saving model to weights-improvement-08-6.3931.hdf5\n",
      "Epoch 9/40\n",
      "104194/104194 [==============================] - 95s 910us/step - loss: 6.3695\n",
      "\n",
      "Epoch 00009: loss improved from 6.39305 to 6.36949, saving model to weights-improvement-09-6.3695.hdf5\n",
      "Epoch 10/40\n",
      "104194/104194 [==============================] - 95s 908us/step - loss: 6.3431\n",
      "\n",
      "Epoch 00010: loss improved from 6.36949 to 6.34314, saving model to weights-improvement-10-6.3431.hdf5\n",
      "Epoch 11/40\n",
      "104194/104194 [==============================] - 93s 894us/step - loss: 6.3138\n",
      "\n",
      "Epoch 00011: loss improved from 6.34314 to 6.31384, saving model to weights-improvement-11-6.3138.hdf5\n",
      "Epoch 12/40\n",
      "104194/104194 [==============================] - 92s 885us/step - loss: 6.2780\n",
      "\n",
      "Epoch 00012: loss improved from 6.31384 to 6.27796, saving model to weights-improvement-12-6.2780.hdf5\n",
      "Epoch 13/40\n",
      "104194/104194 [==============================] - 92s 882us/step - loss: 6.2386\n",
      "\n",
      "Epoch 00013: loss improved from 6.27796 to 6.23858, saving model to weights-improvement-13-6.2386.hdf5\n",
      "Epoch 14/40\n",
      "104194/104194 [==============================] - 92s 881us/step - loss: 6.1901\n",
      "\n",
      "Epoch 00014: loss improved from 6.23858 to 6.19015, saving model to weights-improvement-14-6.1901.hdf5\n",
      "Epoch 15/40\n",
      "104194/104194 [==============================] - 92s 880us/step - loss: 6.1324\n",
      "\n",
      "Epoch 00015: loss improved from 6.19015 to 6.13238, saving model to weights-improvement-15-6.1324.hdf5\n",
      "Epoch 16/40\n",
      "104194/104194 [==============================] - 92s 878us/step - loss: 6.0666\n",
      "\n",
      "Epoch 00016: loss improved from 6.13238 to 6.06664, saving model to weights-improvement-16-6.0666.hdf5\n",
      "Epoch 17/40\n",
      "104194/104194 [==============================] - 91s 877us/step - loss: 5.9914\n",
      "\n",
      "Epoch 00017: loss improved from 6.06664 to 5.99143, saving model to weights-improvement-17-5.9914.hdf5\n",
      "Epoch 18/40\n",
      "104194/104194 [==============================] - 91s 876us/step - loss: 5.9080\n",
      "\n",
      "Epoch 00018: loss improved from 5.99143 to 5.90800, saving model to weights-improvement-18-5.9080.hdf5\n",
      "Epoch 19/40\n",
      "104194/104194 [==============================] - 91s 874us/step - loss: 5.8196\n",
      "\n",
      "Epoch 00019: loss improved from 5.90800 to 5.81963, saving model to weights-improvement-19-5.8196.hdf5\n",
      "Epoch 20/40\n",
      "104194/104194 [==============================] - 91s 874us/step - loss: 5.7266\n",
      "\n",
      "Epoch 00020: loss improved from 5.81963 to 5.72664, saving model to weights-improvement-20-5.7266.hdf5\n",
      "Epoch 21/40\n",
      "104194/104194 [==============================] - 91s 874us/step - loss: 5.6308\n",
      "\n",
      "Epoch 00021: loss improved from 5.72664 to 5.63077, saving model to weights-improvement-21-5.6308.hdf5\n",
      "Epoch 22/40\n",
      "104194/104194 [==============================] - 91s 872us/step - loss: 5.5403\n",
      "\n",
      "Epoch 00022: loss improved from 5.63077 to 5.54029, saving model to weights-improvement-22-5.5403.hdf5\n",
      "Epoch 23/40\n",
      "104194/104194 [==============================] - 91s 873us/step - loss: 5.4510\n",
      "\n",
      "Epoch 00023: loss improved from 5.54029 to 5.45100, saving model to weights-improvement-23-5.4510.hdf5\n",
      "Epoch 24/40\n",
      "104194/104194 [==============================] - 91s 874us/step - loss: 5.3708\n",
      "\n",
      "Epoch 00024: loss improved from 5.45100 to 5.37083, saving model to weights-improvement-24-5.3708.hdf5\n",
      "Epoch 25/40\n",
      "104194/104194 [==============================] - 91s 874us/step - loss: 5.2909\n",
      "\n",
      "Epoch 00025: loss improved from 5.37083 to 5.29088, saving model to weights-improvement-25-5.2909.hdf5\n",
      "Epoch 26/40\n",
      "104194/104194 [==============================] - 91s 875us/step - loss: 5.2123\n",
      "\n",
      "Epoch 00026: loss improved from 5.29088 to 5.21232, saving model to weights-improvement-26-5.2123.hdf5\n",
      "Epoch 27/40\n",
      "104194/104194 [==============================] - 91s 872us/step - loss: 5.1355\n",
      "\n",
      "Epoch 00027: loss improved from 5.21232 to 5.13554, saving model to weights-improvement-27-5.1355.hdf5\n",
      "Epoch 28/40\n",
      "104194/104194 [==============================] - 91s 869us/step - loss: 5.0716\n",
      "\n",
      "Epoch 00028: loss improved from 5.13554 to 5.07159, saving model to weights-improvement-28-5.0716.hdf5\n",
      "Epoch 29/40\n",
      "104194/104194 [==============================] - 90s 868us/step - loss: 4.9979\n",
      "\n",
      "Epoch 00029: loss improved from 5.07159 to 4.99789, saving model to weights-improvement-29-4.9979.hdf5\n",
      "Epoch 30/40\n",
      "104194/104194 [==============================] - 90s 867us/step - loss: 4.9408\n",
      "\n",
      "Epoch 00030: loss improved from 4.99789 to 4.94076, saving model to weights-improvement-30-4.9408.hdf5\n",
      "Epoch 31/40\n",
      "104194/104194 [==============================] - 90s 866us/step - loss: 4.8711\n",
      "\n",
      "Epoch 00031: loss improved from 4.94076 to 4.87115, saving model to weights-improvement-31-4.8711.hdf5\n",
      "Epoch 32/40\n",
      "104194/104194 [==============================] - 91s 870us/step - loss: 4.8124\n",
      "\n",
      "Epoch 00032: loss improved from 4.87115 to 4.81238, saving model to weights-improvement-32-4.8124.hdf5\n",
      "Epoch 33/40\n",
      "104194/104194 [==============================] - 90s 866us/step - loss: 4.7554\n",
      "\n",
      "Epoch 00033: loss improved from 4.81238 to 4.75541, saving model to weights-improvement-33-4.7554.hdf5\n",
      "Epoch 34/40\n",
      "104194/104194 [==============================] - 90s 867us/step - loss: 4.7020\n",
      "\n",
      "Epoch 00034: loss improved from 4.75541 to 4.70197, saving model to weights-improvement-34-4.7020.hdf5\n",
      "Epoch 35/40\n",
      "104194/104194 [==============================] - 90s 864us/step - loss: 4.6425\n",
      "\n",
      "Epoch 00035: loss improved from 4.70197 to 4.64246, saving model to weights-improvement-35-4.6425.hdf5\n",
      "Epoch 36/40\n",
      "104194/104194 [==============================] - 90s 867us/step - loss: 4.5926\n",
      "\n",
      "Epoch 00036: loss improved from 4.64246 to 4.59257, saving model to weights-improvement-36-4.5926.hdf5\n",
      "Epoch 37/40\n",
      "104194/104194 [==============================] - 91s 876us/step - loss: 4.5372\n",
      "\n",
      "Epoch 00037: loss improved from 4.59257 to 4.53719, saving model to weights-improvement-37-4.5372.hdf5\n",
      "Epoch 38/40\n",
      "104194/104194 [==============================] - 90s 867us/step - loss: 4.4928\n",
      "\n",
      "Epoch 00038: loss improved from 4.53719 to 4.49276, saving model to weights-improvement-38-4.4928.hdf5\n",
      "Epoch 39/40\n",
      "104194/104194 [==============================] - 90s 867us/step - loss: 4.4420\n",
      "\n",
      "Epoch 00039: loss improved from 4.49276 to 4.44201, saving model to weights-improvement-39-4.4420.hdf5\n",
      "Epoch 40/40\n",
      "104194/104194 [==============================] - 90s 868us/step - loss: 4.4013\n",
      "\n",
      "Epoch 00040: loss improved from 4.44201 to 4.40133, saving model to weights-improvement-40-4.4013.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1138af048>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=40, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-40-4.4013.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" i have just time with your assistance to \"\n",
      "the the the chairs the the bred the i persuade crawl snake the the have of the the the the the the the justified the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "int_to_token = dict((i, c) for i, c in enumerate(words))\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ' '.join([int_to_token[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(words))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_token[index]\n",
    "    seq_in = [int_to_token[value] for value in pattern]\n",
    "    sys.stdout.write(result + \" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" large towns were within that radius so the \"\n",
      "the win the the the justified the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start2 = numpy.random.randint(576, len(dataX)-1)\n",
    "pattern = dataX[start2]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ' '.join([int_to_token[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(words))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_token[index]\n",
    "    seq_in = [int_to_token[value] for value in pattern]\n",
    "    sys.stdout.write(result + \" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" way for the presence of the missing gentlemans \"\n",
      "sparkled the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start3 = numpy.random.randint(1291, len(dataX)-1)\n",
    "pattern = dataX[start3]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ' '.join([int_to_token[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(words))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_token[index]\n",
    "    seq_in = [int_to_token[value] for value in pattern]\n",
    "    sys.stdout.write(result + \" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" beneath the hanging lamp now then mr cocksure \"\n",
      "i the the i the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start4 = numpy.random.randint(291, len(dataX)-1)\n",
    "pattern = dataX[start4]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ' '.join([int_to_token[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(words))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_token[index]\n",
    "    seq_in = [int_to_token[value] for value in pattern]\n",
    "    sys.stdout.write(result + \" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" st jamess evening news standard echo and any \"\n",
      "small the the the marry the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the the the marrying the the the the \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start5 = numpy.random.randint(2291, len(dataX)-1)\n",
    "pattern = dataX[start5]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ' '.join([int_to_token[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(words))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_token[index]\n",
    "    seq_in = [int_to_token[value] for value in pattern]\n",
    "    sys.stdout.write(result + \" \")\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
