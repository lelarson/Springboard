{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP1 Baseline notebook\n",
    "\n",
    "### By Logan Larson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import packages ###\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "### load data ###\n",
    "\n",
    "df = pd.read_csv('clean_schefter_tweets')\n",
    "del df['Unnamed: 0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "def make_xy(df, vectorizer=None):   \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(df.Tweet.values.astype('U')) # convert object type to unicode\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = (df.Class == 1).values.astype(np.int)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# define cross-validation score\n",
    "def cv_score(clf, X, y, scorefunc):\n",
    "    result = 0.\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(X): # split data into train/test groups, 5 times\n",
    "        clf.fit(X[train], y[train]) # fit the classifier, passed is as clf.\n",
    "        result += scorefunc(clf, X[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average\n",
    "\n",
    "\n",
    "# define log-likelihood score function\n",
    "def log_likelihood(clf, x, y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    irrelevant = y == 0\n",
    "    relevant = ~irrelevant\n",
    "    return prob[irrelevant, 0].sum() + prob[relevant, 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize before train/test split\n",
    "X, y = make_xy(df)\n",
    "\n",
    "# split dataset into a training and test set\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show train/test classification for both models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Bernoulli Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Bernoulli Naive Bayes baseline classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.963     0.892     0.926     19185\n",
      "           1      0.705     0.882     0.783      5595\n",
      "\n",
      "    accuracy                          0.890     24780\n",
      "   macro avg      0.834     0.887     0.855     24780\n",
      "weighted avg      0.905     0.890     0.894     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.952     0.889     0.919      6391\n",
      "           1      0.690     0.848     0.761      1870\n",
      "\n",
      "    accuracy                          0.879      8261\n",
      "   macro avg      0.821     0.868     0.840      8261\n",
      "weighted avg      0.893     0.879     0.884      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BNB = BernoulliNB().fit(xtrain, ytrain)\n",
    "BNB_train_pred = BNB.predict(xtrain)\n",
    "BNB_test_pred = BNB.predict(xtest)\n",
    "\n",
    "print('\\n Bernoulli Naive Bayes baseline classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, BNB_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, BNB_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Multinomial Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Multinomial Naive Bayes baseline classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.965     0.884     0.923     19185\n",
      "           1      0.692     0.890     0.778      5595\n",
      "\n",
      "    accuracy                          0.886     24780\n",
      "   macro avg      0.828     0.887     0.851     24780\n",
      "weighted avg      0.903     0.886     0.890     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.956     0.878     0.915      6391\n",
      "           1      0.674     0.861     0.756      1870\n",
      "\n",
      "    accuracy                          0.874      8261\n",
      "   macro avg      0.815     0.870     0.836      8261\n",
      "weighted avg      0.892     0.874     0.879      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(xtrain, ytrain)\n",
    "\n",
    "MNBtrain_pred = MNB.predict(xtrain)\n",
    "MNBtest_pred = MNB.predict(xtest)\n",
    "\n",
    "print('\\n Multinomial Naive Bayes baseline classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, MNBtrain_pred, labels=[0,1], digits=3))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, MNBtest_pred, labels=[0,1], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression baseline classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.949     0.972     0.961     19185\n",
      "           1      0.896     0.822     0.857      5595\n",
      "\n",
      "    accuracy                          0.938     24780\n",
      "   macro avg      0.922     0.897     0.909     24780\n",
      "weighted avg      0.937     0.938     0.937     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.929     0.958     0.943      6391\n",
      "           1      0.839     0.749     0.791      1870\n",
      "\n",
      "    accuracy                          0.911      8261\n",
      "   macro avg      0.884     0.853     0.867      8261\n",
      "weighted avg      0.908     0.911     0.909      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression().fit(xtrain, ytrain)\n",
    "LR_train_pred = LR.predict(xtrain)\n",
    "LR_test_pred = LR.predict(xtest)\n",
    "\n",
    "print('\\n Logistic Regression baseline classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, LR_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, LR_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine if there is overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we define overfitting as a discrepancy of 10 points between test and training scores for any of four metrics (accuracy, precision, recall and F1 score) -- for both positive (1) and negative (0) classes -- I have little evidence of overfitting in any of my three models. I therefore have little incentive to perform regularization on any of them. However, there are still improvements that can be made, particularly in terms of both recall (7.3 points) and precision (5.7) for my logistic regression model, so I'll proceed with regularization out of curiosity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine optimal hyperparameters for regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Bernoulli Naive Bayes (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes optimal alpha: 5\n"
     ]
    }
   ],
   "source": [
    "# define the grid of parameters to search over\n",
    "alphas = [.01, .1, 1, 5, 10, 50, 100]\n",
    "min_df = 0.001\n",
    "\n",
    "# find the best value for alpha\n",
    "best_alpha = None\n",
    "_, itest = train_test_split(range(df.shape[0]))\n",
    "mask = np.zeros(df.shape[0], dtype=np.bool)\n",
    "mask[itest] = True\n",
    "maxscore = -np.inf\n",
    "for alpha in alphas:        \n",
    "    vectorizer = CountVectorizer(min_df = min_df)       \n",
    "    Xthis, ythis = make_xy(df, vectorizer)\n",
    "    Xtrainthis = Xthis[mask]\n",
    "    ytrainthis = ythis[mask]\n",
    "    \n",
    "    clf = BernoulliNB(alpha=alpha)\n",
    "    \n",
    "    cvscore = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "    \n",
    "    if cvscore > maxscore:\n",
    "        maxscore = cvscore\n",
    "        BNB_best_alpha = alpha\n",
    "        \n",
    "print('Bernoulli Naive Bayes optimal alpha: {}'.format(BNB_best_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Multinomial Naive Bayes (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes optimal alpha: 10\n"
     ]
    }
   ],
   "source": [
    "# define the grid of parameters to search over\n",
    "alphas = [.01, .1, 1, 5, 10, 50, 100]\n",
    "min_df = 0.001\n",
    "\n",
    "# find the best value for alpha\n",
    "best_alpha = None\n",
    "_, itest = train_test_split(range(df.shape[0]))\n",
    "mask = np.zeros(df.shape[0], dtype=np.bool)\n",
    "mask[itest] = True\n",
    "maxscore = -np.inf\n",
    "for alpha in alphas:        \n",
    "    vectorizer = CountVectorizer(min_df = min_df)       \n",
    "    Xthis, ythis = make_xy(df, vectorizer)\n",
    "    Xtrainthis = Xthis[mask]\n",
    "    ytrainthis = ythis[mask]\n",
    "    \n",
    "    clf = MultinomialNB(alpha=alpha)\n",
    "    \n",
    "    cvscore = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "    \n",
    "    if cvscore > maxscore:\n",
    "        maxscore = cvscore\n",
    "        MNB_best_alpha = alpha\n",
    "        \n",
    "print('Multinomial Naive Bayes optimal alpha: {}'.format(MNB_best_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Logistic Regression (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal C value: 1\n"
     ]
    }
   ],
   "source": [
    "# hypertuning C parameter\n",
    "LR2 = LogisticRegression()\n",
    "parameters = {\"C\": [0.0001, 0.001, 0.1, 1, 2, 3, 4, 5, 10, 100, 1000, 10000]}\n",
    "fitmodel = GridSearchCV(LR2, param_grid=parameters, cv=10, scoring=\"accuracy\").fit(xtrain,ytrain)\n",
    "fitmodel.best_params_, fitmodel.best_score_, fitmodel.cv_results_\n",
    "\n",
    "print('Optimal C value:', fitmodel.best_params_['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build regularized models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Bernoulli Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tuned Bernoulli Naive Bayes classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.947     0.909     0.928     19185\n",
      "           1      0.726     0.825     0.772      5595\n",
      "\n",
      "    accuracy                          0.890     24780\n",
      "   macro avg      0.836     0.867     0.850     24780\n",
      "weighted avg      0.897     0.890     0.893     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.941     0.903     0.922      6391\n",
      "           1      0.709     0.807     0.755      1870\n",
      "\n",
      "    accuracy                          0.881      8261\n",
      "   macro avg      0.825     0.855     0.838      8261\n",
      "weighted avg      0.889     0.881     0.884      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BNB_tuned = BernoulliNB(alpha=BNB_best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "BNB_tuned_train_pred = BNB_tuned.predict(xtrain)\n",
    "BNB_tuned_test_pred = BNB_tuned.predict(xtest)\n",
    "\n",
    "print('\\n Tuned Bernoulli Naive Bayes classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, BNB_tuned_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, BNB_tuned_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Compared to non-regularized model:\n",
    "- Less overfitting in terms of overall accuracy, f1 score and recall\n",
    "- Slightly more overfitting in terms of precision\n",
    "- Overall accuracy improved\n",
    "- Improved precision on positive class for both test and training data\n",
    "- Diminished recall on positive class for both test and training data\n",
    "- Improved f1 on test data, diminished f1 on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Multinomial Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tuned Multinomial Naive Bayes classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.947     0.909     0.928     19185\n",
      "           1      0.726     0.825     0.772      5595\n",
      "\n",
      "    accuracy                          0.890     24780\n",
      "   macro avg      0.836     0.867     0.850     24780\n",
      "weighted avg      0.897     0.890     0.893     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.941     0.903     0.922      6391\n",
      "           1      0.709     0.807     0.755      1870\n",
      "\n",
      "    accuracy                          0.881      8261\n",
      "   macro avg      0.825     0.855     0.838      8261\n",
      "weighted avg      0.889     0.881     0.884      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MNB_tuned = MultinomialNB(alpha=MNB_best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "MNB_tuned_train_pred = MNB_tuned.predict(xtrain)\n",
    "MNB_tuned_test_pred = MNB_tuned.predict(xtest)\n",
    "\n",
    "print('\\n Tuned Multinomial Naive Bayes classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, MNB_tuned_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, MNB_tuned_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Compared to non-regularized model:\n",
    "- Less overfitting in terms of precision, recall, f1 score and overall accuracy\n",
    "- Overall accuracy improved\n",
    "- Precision improved for positive class but diminished for negative class\n",
    "- Recall improved for positive class but diminished for negative class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tuned Logistic Regression classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.949     0.972     0.961     19185\n",
      "           1      0.896     0.822     0.857      5595\n",
      "\n",
      "    accuracy                          0.938     24780\n",
      "   macro avg      0.922     0.897     0.909     24780\n",
      "weighted avg      0.937     0.938     0.937     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.929     0.958     0.943      6391\n",
      "           1      0.839     0.749     0.791      1870\n",
      "\n",
      "    accuracy                          0.911      8261\n",
      "   macro avg      0.884     0.853     0.867      8261\n",
      "weighted avg      0.908     0.911     0.909      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR_tuned = LogisticRegression(C=fitmodel.best_params_['C']).fit(xtrain, ytrain)\n",
    "LR_tuned_train_pred = LR_tuned.predict(xtrain)\n",
    "LR_tuned_test_pred = LR_tuned.predict(xtest)\n",
    "\n",
    "print('\\n Tuned Logistic Regression classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, LR_tuned_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, LR_tuned_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Compared to non-regularized model\"\n",
    "\n",
    "Since the optimal alpha turned out to be the same as the default setting (1), there is no difference between regularized and non-regularized models. Let's instead look at a ridge and lasso classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Logistic regression with L1 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression classifier with L1 penalty: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96     19185\n",
      "           1       0.88      0.80      0.84      5595\n",
      "\n",
      "    accuracy                           0.93     24780\n",
      "   macro avg       0.91      0.89      0.90     24780\n",
      "weighted avg       0.93      0.93      0.93     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94      6391\n",
      "           1       0.84      0.75      0.79      1870\n",
      "\n",
      "    accuracy                           0.91      8261\n",
      "   macro avg       0.89      0.85      0.87      8261\n",
      "weighted avg       0.91      0.91      0.91      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lasso\n",
    "\n",
    "lasso = LogisticRegression(C=1, penalty='l1').fit(xtrain,ytrain)\n",
    "\n",
    "lasso_train_pred = lasso.predict(xtrain)\n",
    "lasso_test_pred = lasso.predict(xtest)\n",
    "\n",
    "print('\\n Logistic Regression classifier with L1 penalty: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, lasso_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, lasso_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared to non-regularized model\n",
    "\n",
    "Performs the same as the baseline model on test data, but performs worse on all metrics on training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c. Logistic regression with L2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression classifier with L2 penalty: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.949     0.972     0.961     19185\n",
      "           1      0.896     0.822     0.857      5595\n",
      "\n",
      "    accuracy                          0.938     24780\n",
      "   macro avg      0.922     0.897     0.909     24780\n",
      "weighted avg      0.937     0.938     0.937     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.929     0.958     0.943      6391\n",
      "           1      0.839     0.749     0.791      1870\n",
      "\n",
      "    accuracy                          0.911      8261\n",
      "   macro avg      0.884     0.853     0.867      8261\n",
      "weighted avg      0.908     0.911     0.909      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ridge\n",
    "\n",
    "ridge = LogisticRegression(C=1, penalty='l2').fit(xtrain,ytrain)\n",
    "\n",
    "ridge_train_pred = ridge.predict(xtrain)\n",
    "ridge_test_pred = ridge.predict(xtest)\n",
    "\n",
    "print('\\n Logistic Regression classifier with L2 penalty: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, ridge_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, ridge_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared to non-regularized model:\n",
    "\n",
    "Performs the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which metric aligns with the business problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation depends on the business problem and whether - in this scenario - we should care most about overall accuracy, precision, recall, or a combination of precision and recall, such as the f1 score.\n",
    "\n",
    "To come to a decision, let's start from the beginning. We defined a positive case (labeled 1) as a newsworthy tweet  while a negative case (labeled 0) as an irrelevant tweet. It follows that a false positive can be considered an irrelevant tweet that was incorrectly classified as newsworthy. In contrast, a false negative could be considered a newsworthy tweet that was incorrectly classified as irrelevant. \n",
    "\n",
    "Since I'd rather have an irrelevant tweet classified as newsworthy (false positive) than have a newsworthy tweet classified as irrelevant (false negative) - and risk users missing potentially important information - I primarily want to minimize the number of false negatives. Thus, I should pick the model that performs best in terms of recall.\n",
    "\n",
    "The primary alternative would be to prioritize precision, and that is less preferable because it wouldn't be the worst thing in the world for an irrelevant tweet to be considered newsworthy since I suspect that potential users of my model could simply ignore any such errors. However, I would lose trust quickly if I failed to recognize the news that matters. Put this way, it doesn't make sense to prioritize either precision or any kind of F score if it means a trade-off in terms of recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In prioritizing recall, I initially prefer the non-regularized Multinomial Naive Bayes model due to its high recall on positive cases within the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
