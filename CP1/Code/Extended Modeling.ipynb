{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP1 Extended Modeling\n",
    "\n",
    "## By Logan Larson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>rt rank best kicker history top maybe top 3. f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>april least one trade made (but one waiting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>giant re-signed restricted free agent guard kevin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@profootballtalk aaron retiring least trying g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>jimmy clausen scheduled fly washington spend s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>trip washington enough, redskin also work jimm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>jaguar re-signed veteran defensive end reggie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>surprise here: bear informed defensive end ale...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                              Tweet\n",
       "0      0  rt rank best kicker history top maybe top 3. f...\n",
       "1      0        april least one trade made (but one waiting\n",
       "2      1  giant re-signed restricted free agent guard kevin\n",
       "3      0  @profootballtalk aaron retiring least trying g...\n",
       "4      0  jimmy clausen scheduled fly washington spend s...\n",
       "5      0  trip washington enough, redskin also work jimm...\n",
       "6      1  jaguar re-signed veteran defensive end reggie ...\n",
       "7      1  surprise here: bear informed defensive end ale..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### import packages ###\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "### load data ###\n",
    "\n",
    "df = pd.read_csv('clean_schefter_tweets')\n",
    "del df['Unnamed: 0']\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "def make_xy(df, vectorizer=None):   \n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            analyzer='word',\n",
    "            ngram_range=(1,1)\n",
    "        )\n",
    "    X = vectorizer.fit_transform(df.Tweet.values.astype('U')) # convert object type to unicode\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = (df.Class == 1).values.astype(np.int)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# define cross-validation score\n",
    "def cv_score(clf, X, y, scorefunc):\n",
    "    result = 0.\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(X): # split data into train/test groups, 5 times\n",
    "        clf.fit(X[train], y[train]) # fit the classifier, passed is as clf.\n",
    "        result += scorefunc(clf, X[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average\n",
    "\n",
    "\n",
    "# define log-likelihood score function\n",
    "def log_likelihood(clf, x, y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    irrelevant = y == 0\n",
    "    relevant = ~irrelevant\n",
    "    return prob[irrelevant, 0].sum() + prob[relevant, 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize before train/test split\n",
    "X, y = make_xy(df)\n",
    "\n",
    "# split dataset into a training and test set\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show train/test classification for both models\n",
    "\n",
    "#### 1. Bernoulli Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Bernoulli Naive Bayes baseline classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.961     0.899     0.929     19185\n",
      "           1      0.716     0.875     0.787      5595\n",
      "\n",
      "    accuracy                          0.893     24780\n",
      "   macro avg      0.838     0.887     0.858     24780\n",
      "weighted avg      0.906     0.893     0.897     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.952     0.893     0.921      6391\n",
      "           1      0.697     0.845     0.764      1870\n",
      "\n",
      "    accuracy                          0.882      8261\n",
      "   macro avg      0.825     0.869     0.843      8261\n",
      "weighted avg      0.894     0.882     0.886      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BNB = BernoulliNB().fit(xtrain, ytrain)\n",
    "BNB_train_pred = BNB.predict(xtrain)\n",
    "BNB_test_pred = BNB.predict(xtest)\n",
    "\n",
    "print('\\n Bernoulli Naive Bayes baseline classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, BNB_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, BNB_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Multinomial Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Multinomial Naive Bayes baseline classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.921     0.957     0.938     19185\n",
      "           1      0.828     0.718     0.769      5595\n",
      "\n",
      "    accuracy                          0.903     24780\n",
      "   macro avg      0.875     0.837     0.854     24780\n",
      "weighted avg      0.900     0.903     0.900     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.912     0.951     0.931      6391\n",
      "           1      0.805     0.684     0.740      1870\n",
      "\n",
      "    accuracy                          0.891      8261\n",
      "   macro avg      0.858     0.818     0.835      8261\n",
      "weighted avg      0.887     0.891     0.888      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(xtrain, ytrain)\n",
    "\n",
    "MNBtrain_pred = MNB.predict(xtrain)\n",
    "MNBtest_pred = MNB.predict(xtest)\n",
    "\n",
    "print('\\n Multinomial Naive Bayes baseline classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, MNBtrain_pred, labels=[0,1], digits=3))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, MNBtest_pred, labels=[0,1], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression baseline classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.931     0.972     0.951     19185\n",
      "           1      0.886     0.755     0.815      5595\n",
      "\n",
      "    accuracy                          0.923     24780\n",
      "   macro avg      0.909     0.863     0.883     24780\n",
      "weighted avg      0.921     0.923     0.920     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.919     0.967     0.943      6391\n",
      "           1      0.864     0.709     0.779      1870\n",
      "\n",
      "    accuracy                          0.909      8261\n",
      "   macro avg      0.891     0.838     0.861      8261\n",
      "weighted avg      0.907     0.909     0.906      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression().fit(xtrain, ytrain)\n",
    "LR_train_pred = LR.predict(xtrain)\n",
    "LR_test_pred = LR.predict(xtest)\n",
    "\n",
    "print('\\n Logistic Regression baseline classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, LR_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, LR_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine if there is overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strictly speaking in terms of recall, we again have little evidence of overfitting in any of our models. However, given the changes seen in regularizing the baseline models, I'll again proceed with regularization to see if recall on positive cases might improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine optimal hyperparameters for regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Bernoulli Naive Bayes (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB optimal alpha: 5\n"
     ]
    }
   ],
   "source": [
    "# define the grid of parameters to search over\n",
    "alphas = [.01, .1, 1, 5, 10, 50, 100]\n",
    "min_df = 0.001\n",
    "\n",
    "# find the best value for alpha\n",
    "best_alpha = None\n",
    "_, itest = train_test_split(range(df.shape[0]))\n",
    "mask = np.zeros(df.shape[0], dtype=np.bool)\n",
    "mask[itest] = True\n",
    "maxscore = -np.inf\n",
    "for alpha in alphas:        \n",
    "    vectorizer = TfidfVectorizer(min_df = min_df)       \n",
    "    Xthis, ythis = make_xy(df, vectorizer)\n",
    "    Xtrainthis = Xthis[mask]\n",
    "    ytrainthis = ythis[mask]\n",
    "    \n",
    "    clf = BernoulliNB(alpha=alpha)\n",
    "    \n",
    "    cvscore = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "    \n",
    "    if cvscore > maxscore:\n",
    "        maxscore = cvscore\n",
    "        BNB_best_alpha = alpha\n",
    "        \n",
    "print('Bernoulli NB optimal alpha: {}'.format(BNB_best_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Multinomial Naive Bayes (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes optimal alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "# define the grid of parameters to search over\n",
    "alphas = [.01, .1, 1, 5, 10, 50, 100]\n",
    "min_df = 0.001\n",
    "\n",
    "# find the best value for alpha\n",
    "best_alpha = None\n",
    "_, itest = train_test_split(range(df.shape[0]))\n",
    "mask = np.zeros(df.shape[0], dtype=np.bool)\n",
    "mask[itest] = True\n",
    "maxscore = -np.inf\n",
    "for alpha in alphas:        \n",
    "    vectorizer = TfidfVectorizer(min_df = min_df)       \n",
    "    Xthis, ythis = make_xy(df, vectorizer)\n",
    "    Xtrainthis = Xthis[mask]\n",
    "    ytrainthis = ythis[mask]\n",
    "    \n",
    "    clf = MultinomialNB(alpha=alpha)\n",
    "    \n",
    "    cvscore = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "    \n",
    "    if cvscore > maxscore:\n",
    "        maxscore = cvscore\n",
    "        MNB_best_alpha = alpha\n",
    "        \n",
    "print('Multinomial Naive Bayes optimal alpha: {}'.format(MNB_best_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Logistic Regression (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal C value: 2\n"
     ]
    }
   ],
   "source": [
    "# hypertuning C parameter\n",
    "LR2 = LogisticRegression()\n",
    "parameters = {\"C\": [0.0001, 0.001, 0.1, 1, 2, 3, 4, 5, 10, 100, 1000, 10000]}\n",
    "fitmodel = GridSearchCV(LR2, param_grid=parameters, cv=10, scoring=\"accuracy\").fit(xtrain,ytrain)\n",
    "fitmodel.best_params_, fitmodel.best_score_, fitmodel.cv_results_\n",
    "\n",
    "print('Optimal C value:', fitmodel.best_params_['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build regularized models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Bernoulli Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tuned Bernoulli Naive Bayes classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.936     0.923     0.930     19185\n",
      "           1      0.749     0.783     0.766      5595\n",
      "\n",
      "    accuracy                          0.892     24780\n",
      "   macro avg      0.842     0.853     0.848     24780\n",
      "weighted avg      0.894     0.892     0.893     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.930     0.921     0.925      6391\n",
      "           1      0.738     0.763     0.750      1870\n",
      "\n",
      "    accuracy                          0.885      8261\n",
      "   macro avg      0.834     0.842     0.838      8261\n",
      "weighted avg      0.887     0.885     0.886      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BNB_tuned = BernoulliNB(alpha=BNB_best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "BNB_tuned_train_pred = BNB_tuned.predict(xtrain)\n",
    "BNB_tuned_test_pred = BNB_tuned.predict(xtest)\n",
    "\n",
    "print('\\n Tuned Bernoulli Naive Bayes classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, BNB_tuned_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, BNB_tuned_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared to non-regularized model\n",
    "\n",
    "We saw improvement in precision, but a significant reduction in recall for the positive class. However, we do see less overfitting in terms of recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Multinomial Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tuned Multinomial Naive Bayes classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.930     0.949     0.939     19185\n",
      "           1      0.811     0.757     0.783      5595\n",
      "\n",
      "    accuracy                          0.905     24780\n",
      "   macro avg      0.871     0.853     0.861     24780\n",
      "weighted avg      0.903     0.905     0.904     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.916     0.945     0.930      6391\n",
      "           1      0.788     0.702     0.743      1870\n",
      "\n",
      "    accuracy                          0.890      8261\n",
      "   macro avg      0.852     0.823     0.836      8261\n",
      "weighted avg      0.887     0.890     0.888      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MNB_tuned = MultinomialNB(alpha=MNB_best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "MNB_tuned_train_pred = MNB_tuned.predict(xtrain)\n",
    "MNB_tuned_test_pred = MNB_tuned.predict(xtest)\n",
    "\n",
    "print('\\n Tuned Multinomial Naive Bayes classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, MNB_tuned_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, MNB_tuned_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Compared to non-regularized model:\n",
    "\n",
    "We see increased recall on both test and training data, but also increased overfitting on the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tuned Logistic Regression classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.939     0.971     0.955     19185\n",
      "           1      0.888     0.785     0.833      5595\n",
      "\n",
      "    accuracy                          0.929     24780\n",
      "   macro avg      0.914     0.878     0.894     24780\n",
      "weighted avg      0.928     0.929     0.928     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.924     0.964     0.943      6391\n",
      "           1      0.855     0.728     0.786      1870\n",
      "\n",
      "    accuracy                          0.911      8261\n",
      "   macro avg      0.890     0.846     0.865      8261\n",
      "weighted avg      0.908     0.911     0.908      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR_tuned = LogisticRegression(C=fitmodel.best_params_['C']).fit(xtrain, ytrain)\n",
    "LR_tuned_train_pred = LR_tuned.predict(xtrain)\n",
    "LR_tuned_test_pred = LR_tuned.predict(xtest)\n",
    "\n",
    "print('\\n Tuned Logistic Regression classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, LR_tuned_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, LR_tuned_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Compared to non-regularized model:\n",
    "We see increased recall performance but more overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with increased n-gram range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine TF-IDF vectorizer\n",
    "def make_xy2(df, vectorizer=None):   \n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            analyzer='word',\n",
    "            ngram_range=(1,5)\n",
    "        )\n",
    "    X = vectorizer.fit_transform(df.Tweet.values.astype('U')) # convert object type to unicode\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = (df.Class == 1).values.astype(np.int)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize before train/test split\n",
    "X, y = make_xy2(df)\n",
    "\n",
    "# split dataset into a training and test set\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Bernoulli naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Naive Bayes classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89     19185\n",
      "           1       1.00      0.18      0.30      5595\n",
      "\n",
      "    accuracy                           0.81     24780\n",
      "   macro avg       0.90      0.59      0.60     24780\n",
      "weighted avg       0.85      0.81      0.76     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88      6391\n",
      "           1       0.98      0.05      0.09      1870\n",
      "\n",
      "    accuracy                           0.78      8261\n",
      "   macro avg       0.88      0.52      0.48      8261\n",
      "weighted avg       0.83      0.78      0.70      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB = BernoulliNB().fit(xtrain, ytrain)\n",
    "NB_train_pred = NB.predict(xtrain)\n",
    "NB_test_pred = NB.predict(xtest)\n",
    "\n",
    "print('\\n Naive Bayes classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, NB_train_pred, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, NB_test_pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Multinomial naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Multinomial Naive Bayes baseline classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.866     1.000     0.928     19185\n",
      "           1      0.998     0.467     0.637      5595\n",
      "\n",
      "    accuracy                          0.880     24780\n",
      "   macro avg      0.932     0.734     0.782     24780\n",
      "weighted avg      0.896     0.880     0.862     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.798     0.999     0.887      6391\n",
      "           1      0.984     0.135     0.238      1870\n",
      "\n",
      "    accuracy                          0.804      8261\n",
      "   macro avg      0.891     0.567     0.563      8261\n",
      "weighted avg      0.840     0.804     0.740      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(xtrain, ytrain)\n",
    "\n",
    "MNBtrain_pred = MNB.predict(xtrain)\n",
    "MNBtest_pred = MNB.predict(xtest)\n",
    "\n",
    "print('\\n Multinomial Naive Bayes baseline classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, MNBtrain_pred, labels=[0,1], digits=3))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, MNBtest_pred, labels=[0,1], digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.913     0.995     0.952     19185\n",
      "           1      0.976     0.676     0.799      5595\n",
      "\n",
      "    accuracy                          0.923     24780\n",
      "   macro avg      0.945     0.836     0.876     24780\n",
      "weighted avg      0.927     0.923     0.918     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.868     0.987     0.924      6391\n",
      "           1      0.918     0.487     0.636      1870\n",
      "\n",
      "    accuracy                          0.874      8261\n",
      "   macro avg      0.893     0.737     0.780      8261\n",
      "weighted avg      0.879     0.874     0.859      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression().fit(xtrain, ytrain)\n",
    "LR_train_pred = LR.predict(xtrain)\n",
    "LR_test_pred = LR.predict(xtest)\n",
    "\n",
    "print('\\n Logistic Regression classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, LR_train_pred, digits=3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, LR_test_pred, digits=3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I am disappointed thus far at the recall performance across the board for this batch of models using the TF-IDF vectorizer. None of them have yet to approach the scores the baseline Naive Bayes (both Bernoulli and Multinomial) models achieved. However, I didn't experiment with any other n-gram ranges using the Count Vectorizer, and I'm curious to do so with the Multinomial NB model -- due to its superior recall (0.861) -- before making any conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Bernoulli Naive Bayes baseline classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.995     0.977     0.986     19185\n",
      "           1      0.926     0.983     0.954      5595\n",
      "\n",
      "    accuracy                          0.978     24780\n",
      "   macro avg      0.961     0.980     0.970     24780\n",
      "weighted avg      0.979     0.978     0.979     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.951     0.889     0.919      6391\n",
      "           1      0.691     0.845     0.760      1870\n",
      "\n",
      "    accuracy                          0.879      8261\n",
      "   macro avg      0.821     0.867     0.840      8261\n",
      "weighted avg      0.892     0.879     0.883      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# redefine Count vectorizer to use n-grams of one to three words\n",
    "\n",
    "def make_xy4(df, vectorizer=None):   \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer(\n",
    "            stop_words='english',\n",
    "            analyzer='word',\n",
    "            ngram_range=(1,3)\n",
    "        )\n",
    "    X = vectorizer.fit_transform(df.Tweet.values.astype('U')) # convert object type to unicode\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = (df.Class == 1).values.astype(np.int)\n",
    "    return X, y\n",
    "\n",
    "# vectorize before train/test split\n",
    "X, y = make_xy4(df)\n",
    "\n",
    "# split dataset into a training and test set\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=1)\n",
    "\n",
    "BNB = MultinomialNB().fit(xtrain, ytrain)\n",
    "BNB_train_pred = BNB.predict(xtrain)\n",
    "BNB_test_pred = BNB.predict(xtest)\n",
    "\n",
    "print('\\n N-gram range: 1-3 \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, BNB_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, BNB_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Bernoulli Naive Bayes baseline classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.999     0.988     0.993     19185\n",
      "           1      0.961     0.995     0.978      5595\n",
      "\n",
      "    accuracy                          0.990     24780\n",
      "   macro avg      0.980     0.992     0.986     24780\n",
      "weighted avg      0.990     0.990     0.990     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.959     0.869     0.912      6391\n",
      "           1      0.661     0.874     0.753      1870\n",
      "\n",
      "    accuracy                          0.870      8261\n",
      "   macro avg      0.810     0.871     0.832      8261\n",
      "weighted avg      0.892     0.870     0.876      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# redefine Count vectorizer to use n-grams of one to four words\n",
    "\n",
    "def make_xy4(df, vectorizer=None):   \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer(\n",
    "            stop_words='english',\n",
    "            analyzer='word',\n",
    "            ngram_range=(1,4)\n",
    "        )\n",
    "    X = vectorizer.fit_transform(df.Tweet.values.astype('U')) # convert object type to unicode\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = (df.Class == 1).values.astype(np.int)\n",
    "    return X, y\n",
    "\n",
    "# vectorize before train/test split\n",
    "X, y = make_xy4(df)\n",
    "\n",
    "# split dataset into a training and test set\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=1)\n",
    "\n",
    "BNB = MultinomialNB().fit(xtrain, ytrain)\n",
    "BNB_train_pred = BNB.predict(xtrain)\n",
    "BNB_test_pred = BNB.predict(xtest)\n",
    "\n",
    "print('\\n N-gram range: 1-4 \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, BNB_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, BNB_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Bernoulli Naive Bayes baseline classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.999     0.993     0.996     19185\n",
      "           1      0.977     0.996     0.986      5595\n",
      "\n",
      "    accuracy                          0.994     24780\n",
      "   macro avg      0.988     0.995     0.991     24780\n",
      "weighted avg      0.994     0.994     0.994     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.965     0.850     0.904      6391\n",
      "           1      0.636     0.894     0.743      1870\n",
      "\n",
      "    accuracy                          0.860      8261\n",
      "   macro avg      0.800     0.872     0.823      8261\n",
      "weighted avg      0.890     0.860     0.867      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# redefine Count vectorizer to use n-grams of one to five words\n",
    "\n",
    "def make_xy4(df, vectorizer=None):   \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer(\n",
    "            stop_words='english',\n",
    "            analyzer='word',\n",
    "            ngram_range=(1,5)\n",
    "        )\n",
    "    X = vectorizer.fit_transform(df.Tweet.values.astype('U')) # convert object type to unicode\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = (df.Class == 1).values.astype(np.int)\n",
    "    return X, y\n",
    "\n",
    "# vectorize before train/test split\n",
    "X, y = make_xy4(df)\n",
    "\n",
    "# split dataset into a training and test set\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=1)\n",
    "\n",
    "BNB = MultinomialNB().fit(xtrain, ytrain)\n",
    "BNB_train_pred = BNB.predict(xtrain)\n",
    "BNB_test_pred = BNB.predict(xtest)\n",
    "\n",
    "print('\\n N-gram range: 1-5 \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, BNB_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, BNB_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Bernoulli Naive Bayes baseline classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.999     0.996     0.997     19185\n",
      "           1      0.986     0.997     0.991      5595\n",
      "\n",
      "    accuracy                          0.996     24780\n",
      "   macro avg      0.992     0.996     0.994     24780\n",
      "weighted avg      0.996     0.996     0.996     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.376     0.542      6391\n",
      "           1      0.310     0.959     0.469      1870\n",
      "\n",
      "    accuracy                          0.508      8261\n",
      "   macro avg      0.640     0.667     0.505      8261\n",
      "weighted avg      0.820     0.508     0.525      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# redefine Count vectorizer to use n-grams of two to five words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def make_xy3(df, vectorizer=None):   \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer(\n",
    "            stop_words='english',\n",
    "            analyzer='word',\n",
    "            ngram_range=(2,5)\n",
    "        )\n",
    "    X = vectorizer.fit_transform(df.Tweet.values.astype('U')) # convert object type to unicode\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = (df.Class == 1).values.astype(np.int)\n",
    "    return X, y\n",
    "\n",
    "# vectorize before train/test split\n",
    "X, y = make_xy3(df)\n",
    "\n",
    "# split dataset into a training and test set\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=1)\n",
    "\n",
    "MNB = MultinomialNB().fit(xtrain, ytrain)\n",
    "BNB_train_pred = BNB.predict(xtrain)\n",
    "BNB_test_pred = BNB.predict(xtest)\n",
    "\n",
    "print('\\n N-gram range: 2-5 \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, BNB_train_pred, digits = 3, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, BNB_test_pred, digits = 3, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This latest multinomial Naive Bayes model that uses a Count Vectorizer and n-grams of range 2 through 5 achieves the highest recall rate yet. The most considerable alternative is a similar model that expands to use n-grams of size 1, which appears to help strike a much better balance between precision and recall. However, as stated earlier, I believe potential users of my model won't have nearly as big of a problem with frequent false positives than they would with only a few false negatives, so for my business problem I'm inclined to maximize recall rate and I thus choose the Multinomial Naive Bayes model using a Count Vectorizer and n-grams of range 2 through 5. While a low precision means I should expect frequent false positives, this model will make up for it by capturing the vast majority of newsworthy tweets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
