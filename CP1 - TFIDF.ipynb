{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP1 TF-IDF notebook\n",
    "\n",
    "## By Logan Larson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>rt rank best kicker history top maybe top 3. f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>april least one trade made (but one waiting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>giant re-signed restricted free agent guard kevin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                              Tweet\n",
       "0      0  rt rank best kicker history top maybe top 3. f...\n",
       "1      0        april least one trade made (but one waiting\n",
       "2      1  giant re-signed restricted free agent guard kevin"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### import packages ###\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "### load data ###\n",
    "\n",
    "df = pd.read_csv('clean_schefter_tweets')\n",
    "del df['Unnamed: 0']\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "def make_xy(df, vectorizer=None):   \n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            analyzer='word',\n",
    "            ngram_range=(1,1)\n",
    "        )\n",
    "    X = vectorizer.fit_transform(df.Tweet.values.astype('U')) # convert object type to unicode\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = (df.Class == 1).values.astype(np.int)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# define cross-validation score\n",
    "def cv_score(clf, X, y, scorefunc):\n",
    "    result = 0.\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(X): # split data into train/test groups, 5 times\n",
    "        clf.fit(X[train], y[train]) # fit the classifier, passed is as clf.\n",
    "        result += scorefunc(clf, X[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average\n",
    "\n",
    "\n",
    "# define log-likelihood score function\n",
    "def log_likelihood(clf, x, y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    irrelevant = y == 0\n",
    "    relevant = ~irrelevant\n",
    "    return prob[irrelevant, 0].sum() + prob[relevant, 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize before train/test split\n",
    "X, y = make_xy(df)\n",
    "\n",
    "# split dataset into a training and test set\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show train/test classification for both models\n",
    "\n",
    "#### 1. Naive Bayes model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Naive Bayes classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93     19185\n",
      "           1       0.72      0.87      0.79      5595\n",
      "\n",
      "    accuracy                           0.89     24780\n",
      "   macro avg       0.84      0.89      0.86     24780\n",
      "weighted avg       0.91      0.89      0.90     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92      6391\n",
      "           1       0.70      0.85      0.76      1870\n",
      "\n",
      "    accuracy                           0.88      8261\n",
      "   macro avg       0.82      0.87      0.84      8261\n",
      "weighted avg       0.89      0.88      0.89      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB = BernoulliNB().fit(xtrain, ytrain)\n",
    "NBtrain_pred = NB.predict(xtrain)\n",
    "NBtest_pred = NB.predict(xtest)\n",
    "\n",
    "print('\\n Naive Bayes classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, NBtrain_pred, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, NBtest_pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95     19185\n",
      "           1       0.89      0.75      0.82      5595\n",
      "\n",
      "    accuracy                           0.92     24780\n",
      "   macro avg       0.91      0.86      0.88     24780\n",
      "weighted avg       0.92      0.92      0.92     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94      6391\n",
      "           1       0.86      0.71      0.78      1870\n",
      "\n",
      "    accuracy                           0.91      8261\n",
      "   macro avg       0.89      0.84      0.86      8261\n",
      "weighted avg       0.91      0.91      0.91      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression().fit(xtrain, ytrain)\n",
    "LR_train_pred = LR.predict(xtrain)\n",
    "LR_test_pred = LR.predict(xtest)\n",
    "\n",
    "print('\\n Logistic Regression classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, LR_train_pred, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, LR_test_pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine if there is overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the baseline notebook, I determined precision to be the metric of focus. While the naive bayes classifier is still overfit in these terms, it's \"less overfit\" due to higher precision on positive observations on both test and training data. On the other hand, the logistic regression model performed similarly and is also \"less overfit\". Plus, the precision on positive observations on the test data improved. Since the naive bayes classifier is the only model that was overfit in terms of precision, I won't bother making a regularized model for the logistic regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build regularized models\n",
    "\n",
    "#### 1. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 5\n"
     ]
    }
   ],
   "source": [
    "# define the grid of parameters to search over\n",
    "alphas = [.01, .1, 1, 5, 10, 50, 100]\n",
    "min_df = 0.001\n",
    "\n",
    "# find the best value for alpha\n",
    "best_alpha = None\n",
    "_, itest = train_test_split(range(df.shape[0]))\n",
    "mask = np.zeros(df.shape[0], dtype=np.bool)\n",
    "mask[itest] = True\n",
    "maxscore = -np.inf\n",
    "for alpha in alphas:        \n",
    "    vectorizer = TfidfVectorizer(min_df = min_df)       \n",
    "    Xthis, ythis = make_xy(df, vectorizer)\n",
    "    Xtrainthis = Xthis[mask]\n",
    "    ytrainthis = ythis[mask]\n",
    "    \n",
    "    clf = BernoulliNB(alpha=alpha)\n",
    "    \n",
    "    cvscore = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "    \n",
    "    if cvscore > maxscore:\n",
    "        maxscore = cvscore\n",
    "        best_alpha = alpha\n",
    "        \n",
    "print('Optimal alpha: {}'.format(best_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tuned Naive Bayes classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     19185\n",
      "           1       0.75      0.78      0.77      5595\n",
      "\n",
      "    accuracy                           0.89     24780\n",
      "   macro avg       0.84      0.85      0.85     24780\n",
      "weighted avg       0.89      0.89      0.89     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93      6391\n",
      "           1       0.74      0.76      0.75      1870\n",
      "\n",
      "    accuracy                           0.89      8261\n",
      "   macro avg       0.83      0.84      0.84      8261\n",
      "weighted avg       0.89      0.89      0.89      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_tuned = BernoulliNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "NB_tuned_train_pred = NB_tuned.predict(xtrain)\n",
    "NB_tuned_test_pred = NB_tuned.predict(xtest)\n",
    "\n",
    "print('\\n Tuned Naive Bayes classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, NB_tuned_train_pred, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, NB_tuned_test_pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared to non-regularized model\n",
    "\n",
    "This performs better than the non-regularized model, but it doesn't compare to the logistic regression model. I'll proceed with another logistic model using n-grams greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine vectorizer\n",
    "def make_xy2(df, vectorizer=None):   \n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            analyzer='word',\n",
    "            ngram_range=(1,4)\n",
    "        )\n",
    "    X = vectorizer.fit_transform(df.Tweet.values.astype('U')) # convert object type to unicode\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = (df.Class == 1).values.astype(np.int)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize before train/test split\n",
    "X, y = make_xy2(df)\n",
    "\n",
    "# split dataset into a training and test set\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression classifier: \n",
      " \n",
      "\n",
      "\n",
      " Training Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     19185\n",
      "           1       0.97      0.70      0.81      5595\n",
      "\n",
      "    accuracy                           0.93     24780\n",
      "   macro avg       0.94      0.85      0.88     24780\n",
      "weighted avg       0.93      0.93      0.92     24780\n",
      "\n",
      "\n",
      " Test Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93      6391\n",
      "           1       0.91      0.53      0.67      1870\n",
      "\n",
      "    accuracy                           0.88      8261\n",
      "   macro avg       0.89      0.76      0.80      8261\n",
      "weighted avg       0.89      0.88      0.87      8261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression().fit(xtrain, ytrain)\n",
    "LR_train_pred = LR.predict(xtrain)\n",
    "LR_test_pred = LR.predict(xtest)\n",
    "\n",
    "print('\\n Logistic Regression classifier: \\n \\n')\n",
    "print('\\n Training Classification Report: \\n', classification_report(ytrain, LR_train_pred, labels=[0,1]))\n",
    "print('\\n Test Classification Report: \\n', classification_report(ytest, LR_test_pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is performing better on positive cases than negative ones and achieved the highest precision yet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
